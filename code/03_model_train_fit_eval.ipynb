{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "081925ab-eda3-4bc0-a8ef-14e7fff17bc3",
   "metadata": {},
   "source": [
    "# 03: Model Training, Fitting, and Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e619b096-0997-47a7-8354-0dee156b10b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import shap     # a game theoretical approach to explaining model output https://shap.readthedocs.io/en/latest/index.html\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cbd58aef-a89f-422e-aa72-2a42546242ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training process...\n",
      "Successfully loaded data with 331 rows and 35 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting model training process...\")\n",
    "\n",
    "# Load the cleaned data\n",
    "try:\n",
    "    df = pd.read_csv('./data/cleaned_fatalities.csv')\n",
    "    print(f\"Successfully loaded data with {len(df)} rows and {len(df.columns)} columns\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cdb21b79-0d27-4b8e-bc16-c7773af714d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data validation passed. Beginning feature engineering...\n"
     ]
    }
   ],
   "source": [
    "# Validate required columns\n",
    "required_columns = ['latitude', 'longitude', 'collision_category', 'time_of_day']\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    print(f\"Error: Missing required columns: {missing_columns}\")\n",
    "    print(\"Available columns:\", df.columns.tolist())\n",
    "    exit(1)\n",
    "\n",
    "print(\"Data validation passed. Beginning feature engineering...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f901d9c2-a8ca-43ef-bb7f-62b3664d6160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating day_of_week column...\n",
      "Successfully created day_of_week from collision_date\n",
      "day_of_week column exists: True\n",
      "Unique values in day_of_week: ['Thursday' 'Monday' 'Saturday' 'Sunday' 'Wednesday' 'Friday' 'Tuesday']\n",
      "NaN values in day_of_week: 0\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "# Add day of week with robust error handling\n",
    "print(\"\\nCreating day_of_week column...\")\n",
    "\n",
    "# First try using collision_date\n",
    "if 'collision_date' in df.columns:\n",
    "    try:\n",
    "        df['collision_date'] = pd.to_datetime(df['collision_date'], errors='coerce')\n",
    "        if df['collision_date'].notna().sum() > len(df) * 0.5:  # If more than 50% valid dates\n",
    "            df['day_of_week'] = df['collision_date'].dt.day_name()\n",
    "            print(\"Successfully created day_of_week from collision_date\")\n",
    "        else:\n",
    "            raise ValueError(\"Too many NaN values in collision_date\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error using collision_date: {e}\")\n",
    "        # Try alternative method\n",
    "        if all(col in df.columns for col in ['collision_year_clean', 'collision_month', 'collision_day']):\n",
    "            try:\n",
    "                df['synthetic_date'] = pd.to_datetime(\n",
    "                    df['collision_year_clean'].astype(str) + '-' + \n",
    "                    df['collision_month'].astype(str) + '-' + \n",
    "                    df['collision_day'].astype(str),\n",
    "                    errors='coerce'\n",
    "                )\n",
    "                df['day_of_week'] = df['synthetic_date'].dt.day_name()\n",
    "                print(\"Successfully created day_of_week from synthetic date\")\n",
    "            except Exception as e2:\n",
    "                print(f\"Error creating synthetic date: {e2}\")\n",
    "                # Fall back to default\n",
    "                days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "                df['day_of_week'] = [days[i % 7] for i in range(len(df))]\n",
    "                print(\"Created default day_of_week\")\n",
    "        else:\n",
    "            # Fall back to default\n",
    "            days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "            df['day_of_week'] = [days[i % 7] for i in range(len(df))]\n",
    "            print(\"Created default day_of_week\")\n",
    "else:\n",
    "    # If collision_date doesn't exist, create a default day_of_week\n",
    "    print(\"Warning: collision_date column not found\")\n",
    "    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    df['day_of_week'] = [days[i % 7] for i in range(len(df))]\n",
    "    print(\"Created default day_of_week\")# Verify day_of_week column\n",
    "print(f\"day_of_week column exists: {'day_of_week' in df.columns}\")\n",
    "if 'day_of_week' in df.columns:\n",
    "    print(f\"Unique values in day_of_week: {df['day_of_week'].unique()}\")\n",
    "    print(f\"NaN values in day_of_week: {df['day_of_week'].isna().sum()}\")\n",
    "    # Fill any NaN values\n",
    "    df['day_of_week'] = df['day_of_week'].fillna('Monday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8229cd2e-70a6-47eb-9569-469746f97b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating target variable...\n",
      "Using direct approach to create high_risk target variable\n",
      "High risk incidents: 247 out of 331 total records\n",
      "Percentage high risk: 74.62%\n",
      "\n",
      "Preparing features...\n"
     ]
    }
   ],
   "source": [
    "# Create target variable: high-risk vs low-risk areas\n",
    "print(\"\\nCreating target variable...\")\n",
    "\n",
    "# SIMPLIFIED APPROACH: Define high-risk based on collision category and time of day\n",
    "# This avoids the merge operation that was causing issues\n",
    "print(\"Using direct approach to create high_risk target variable\")\n",
    "df['high_risk'] = ((df['collision_category'] == 'Pedestrian') | \n",
    "                  (df['time_of_day'] == 'Night (9pm-5am)')).astype(int)\n",
    "\n",
    "# Verify the column exists and has appropriate values\n",
    "print(f\"High risk incidents: {df['high_risk'].sum()} out of {len(df)} total records\")\n",
    "print(f\"Percentage high risk: {df['high_risk'].mean() * 100:.2f}%\")\n",
    "\n",
    "# Select features for the model\n",
    "print(\"\\nPreparing features...\")\n",
    "features = [\n",
    "    'latitude', 'longitude', 'collision_hour', 'collision_month',\n",
    "    'time_of_day', 'day_of_week', 'collision_category'\n",
    "]\n",
    "\n",
    "# Check if all features exist\n",
    "missing_features = [f for f in features if f not in df.columns]\n",
    "if missing_features:\n",
    "    print(f\"Warning: Missing features: {missing_features}\")\n",
    "    # Remove missing features from the list\n",
    "    features = [f for f in features if f in df.columns]\n",
    "    print(f\"Proceeding with available features: {features}\")\n",
    "\n",
    "# Handle missing values in features\n",
    "for feature in features:\n",
    "    if df[feature].dtype == 'object':\n",
    "        df[feature] = df[feature].fillna('Unknown')\n",
    "    else:\n",
    "        df[feature] = df[feature].fillna(df[feature].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4b1921d5-0e09-4e31-a589-2af835f0228a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution:\n",
      "high_risk\n",
      "1    247\n",
      "0     84\n",
      "Name: count, dtype: int64\n",
      "Class balance: 74.62% high risk\n"
     ]
    }
   ],
   "source": [
    "# Prepare X and y\n",
    "X = df[features]\n",
    "y = df['high_risk']\n",
    "\n",
    "# Print class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"Class balance: {y.mean() * 100:.2f}% high risk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1f64427c-0ba0-49c9-9c52-9acdec027b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting data into train and test sets...\n",
      "Class distribution in training set:\n",
      "high_risk\n",
      "1    185\n",
      "0     63\n",
      "Name: count, dtype: int64\n",
      "Training set class balance: 74.60% high risk\n",
      "\n",
      "Building preprocessing pipeline...\n",
      "Numerical features: ['latitude', 'longitude', 'collision_hour', 'collision_month']\n",
      "Categorical features: ['time_of_day', 'day_of_week', 'collision_category']\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "print(\"\\nSplitting data into train and test sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "# Check class distribution in training set\n",
    "print(\"Class distribution in training set:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "print(f\"Training set class balance: {y_train.mean() * 100:.2f}% high risk\")\n",
    "\n",
    "# Define preprocessing for numerical and categorical features\n",
    "print(\"\\nBuilding preprocessing pipeline...\")\n",
    "numerical_features = [f for f in features if df[f].dtype != 'object']\n",
    "categorical_features = [f for f in features if df[f].dtype == 'object']\n",
    "\n",
    "print(f\"Numerical features: {numerical_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "273f9fc8-6089-4554-be52-da13693e3603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the model...\n",
      "Model training completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Create and train the model\n",
    "print(\"\\nTraining the model...\")\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "try:\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Model training completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during model training: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8d224932-76a5-4782-89a3-aaef16623054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model performance...\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        62\n",
      "\n",
      "    accuracy                           1.00        83\n",
      "   macro avg       1.00      1.00      1.00        83\n",
      "weighted avg       1.00      1.00      1.00        83\n",
      "\n",
      "ROC AUC Score: 1.0000\n",
      "\n",
      "Generating SHAP values for feature importance...\n",
      "Generated SHAP values with 18 features\n",
      "SHAP values saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print(\"\\nEvaluating model performance...\")\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "try:\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not calculate ROC AUC: {e}\")\n",
    "\n",
    "# Generate SHAP values for feature importance\n",
    "print(\"\\nGenerating SHAP values for feature importance...\")\n",
    "try:\n",
    "    # Get the preprocessed test data\n",
    "    X_test_processed = model.named_steps['preprocessor'].transform(X_test)\n",
    "    \n",
    "    # Create a SHAP explainer\n",
    "    explainer = shap.TreeExplainer(model.named_steps['classifier'])\n",
    "    shap_values = explainer.shap_values(X_test_processed)\n",
    "    \n",
    "    # Get feature names after preprocessing\n",
    "    cat_feature_names = []\n",
    "    for i, col in enumerate(categorical_features):\n",
    "        cats = model.named_steps['preprocessor'].transformers_[1][1].categories_[i]\n",
    "        for cat in cats:\n",
    "            cat_feature_names.append(f\"{col}_{cat}\")\n",
    "    \n",
    "    feature_names = numerical_features + cat_feature_names\n",
    "    \n",
    "    print(f\"Generated SHAP values with {len(feature_names)} features\")\n",
    "    \n",
    "    # Save feature names for the Streamlit app\n",
    "    with open('./code/feature_names.pkl', 'wb') as f:\n",
    "        pickle.dump(feature_names, f)\n",
    "    \n",
    "    # Save SHAP values for the Streamlit app\n",
    "    with open('./code/shap_values.pkl', 'wb') as f:\n",
    "        pickle.dump((shap_values, X_test_processed), f)\n",
    "    \n",
    "    print(\"SHAP values saved successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error generating SHAP values: {e}\")\n",
    "    print(\"Continuing without SHAP values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0d83225c-b9ad-4b18-8c92-acfb292c5a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model and data for Streamlit app...\n",
      "Model saved successfully\n",
      "Processed data saved successfully\n",
      "\n",
      "Model training complete. Files saved for Streamlit app.\n",
      "You can now run the Streamlit app with: streamlit run app.py\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "print(\"\\nSaving model and data for Streamlit app...\")\n",
    "try:\n",
    "    with open('./code/fatality_risk_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(\"Model saved successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Save a sample of the data for the Streamlit app\n",
    "try:\n",
    "    df_sample = df.sample(frac=1.0, random_state=42)\n",
    "    df_sample.to_csv('./data/fatality_data_processed.csv', index=False)\n",
    "    print(\"Processed data saved successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving processed data: {e}\")\n",
    "\n",
    "print(\"\\nModel training complete. Files saved for Streamlit app.\")\n",
    "print(\"You can now run the Streamlit app with: streamlit run app.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
