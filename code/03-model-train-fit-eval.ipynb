{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "081925ab-eda3-4bc0-a8ef-14e7fff17bc3",
   "metadata": {},
   "source": [
    "# 03: Model Training, Fitting, and Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e619b096-0997-47a7-8354-0dee156b10b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import shap     # a game theoretical approach to explaining model output https://shap.readthedocs.io/en/latest/index.html\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbd58aef-a89f-422e-aa72-2a42546242ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training process...\n",
      "Successfully loaded data with 331 rows and 35 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting model training process...\")\n",
    "\n",
    "# Load the cleaned data\n",
    "try:\n",
    "    df = pd.read_csv('../data/cleaned_fatalities.csv')\n",
    "    print(f\"Successfully loaded data with {len(df)} rows and {len(df.columns)} columns\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdb21b79-0d27-4b8e-bc16-c7773af714d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data validation passed. Beginning feature engineering...\n"
     ]
    }
   ],
   "source": [
    "# Validate required columns\n",
    "required_columns = ['latitude', 'longitude', 'collision_category', 'time_of_day']\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    print(f\"Error: Missing required columns: {missing_columns}\")\n",
    "    print(\"Available columns:\", df.columns.tolist())\n",
    "    exit(1)\n",
    "\n",
    "print(\"Data validation passed. Beginning feature engineering...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f901d9c2-a8ca-43ef-bb7f-62b3664d6160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating day_of_week column...\n",
      "Successfully created day_of_week from collision_date\n",
      "day_of_week column exists: True\n",
      "Unique values in day_of_week: ['Thursday' 'Monday' 'Saturday' 'Sunday' 'Wednesday' 'Friday' 'Tuesday']\n",
      "NaN values in day_of_week: 0\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "# Add day of week with robust error handling\n",
    "print(\"\\nCreating day_of_week column...\")\n",
    "\n",
    "# First try using collision_date\n",
    "if 'collision_date' in df.columns:\n",
    "    try:\n",
    "        df['collision_date'] = pd.to_datetime(df['collision_date'], errors='coerce')\n",
    "        if df['collision_date'].notna().sum() > len(df) * 0.5:  # If more than 50% valid dates\n",
    "            df['day_of_week'] = df['collision_date'].dt.day_name()\n",
    "            print(\"Successfully created day_of_week from collision_date\")\n",
    "        else:\n",
    "            raise ValueError(\"Too many NaN values in collision_date\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error using collision_date: {e}\")\n",
    "        # Try alternative method\n",
    "        if all(col in df.columns for col in ['collision_year_clean', 'collision_month', 'collision_day']):\n",
    "            try:\n",
    "                df['synthetic_date'] = pd.to_datetime(\n",
    "                    df['collision_year_clean'].astype(str) + '-' + \n",
    "                    df['collision_month'].astype(str) + '-' + \n",
    "                    df['collision_day'].astype(str),\n",
    "                    errors='coerce'\n",
    "                )\n",
    "                df['day_of_week'] = df['synthetic_date'].dt.day_name()\n",
    "                print(\"Successfully created day_of_week from synthetic date\")\n",
    "            except Exception as e2:\n",
    "                print(f\"Error creating synthetic date: {e2}\")\n",
    "                # Fall back to default\n",
    "                days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "                df['day_of_week'] = [days[i % 7] for i in range(len(df))]\n",
    "                print(\"Created default day_of_week\")\n",
    "        else:\n",
    "            # Fall back to default\n",
    "            days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "            df['day_of_week'] = [days[i % 7] for i in range(len(df))]\n",
    "            print(\"Created default day_of_week\")\n",
    "else:\n",
    "    # If collision_date doesn't exist, create a default day_of_week\n",
    "    print(\"Warning: collision_date column not found\")\n",
    "    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    df['day_of_week'] = [days[i % 7] for i in range(len(df))]\n",
    "    print(\"Created default day_of_week\")# Verify day_of_week column\n",
    "print(f\"day_of_week column exists: {'day_of_week' in df.columns}\")\n",
    "if 'day_of_week' in df.columns:\n",
    "    print(f\"Unique values in day_of_week: {df['day_of_week'].unique()}\")\n",
    "    print(f\"NaN values in day_of_week: {df['day_of_week'].isna().sum()}\")\n",
    "    # Fill any NaN values\n",
    "    df['day_of_week'] = df['day_of_week'].fillna('Monday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d114c0d-4400-480b-8919-542cc9eed2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>case_id_fkey</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>collision_year</th>\n",
       "      <th>death_date</th>\n",
       "      <th>death_time</th>\n",
       "      <th>death_datetime</th>\n",
       "      <th>collision_date</th>\n",
       "      <th>collision_time</th>\n",
       "      <th>...</th>\n",
       "      <th>data_as_of</th>\n",
       "      <th>data_loaded_at</th>\n",
       "      <th>age_category</th>\n",
       "      <th>collision_category</th>\n",
       "      <th>collision_hour</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>collision_year_clean</th>\n",
       "      <th>collision_month</th>\n",
       "      <th>collision_day</th>\n",
       "      <th>day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>140236301</td>\n",
       "      <td>37.710409</td>\n",
       "      <td>-122.404226</td>\n",
       "      <td>2014</td>\n",
       "      <td>2014-03-20</td>\n",
       "      <td>11:21:00</td>\n",
       "      <td>2014-03-20 11:21:00</td>\n",
       "      <td>2014-03-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-09-18 12:00:00</td>\n",
       "      <td>2025-04-08 04:35:01</td>\n",
       "      <td>Senior (65+)</td>\n",
       "      <td>Pedestrian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afternoon (12pm-5pm)</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>140755533</td>\n",
       "      <td>37.725476</td>\n",
       "      <td>-122.394243</td>\n",
       "      <td>2014</td>\n",
       "      <td>2014-09-08</td>\n",
       "      <td>16:38:00</td>\n",
       "      <td>2014-09-08 04:38:00</td>\n",
       "      <td>2014-09-08</td>\n",
       "      <td>05:10:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-09-18 12:00:00</td>\n",
       "      <td>2025-04-08 04:35:01</td>\n",
       "      <td>Senior (65+)</td>\n",
       "      <td>Pedestrian</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Morning (5am-12pm)</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>140365546</td>\n",
       "      <td>37.748255</td>\n",
       "      <td>-122.413669</td>\n",
       "      <td>2014</td>\n",
       "      <td>2014-05-03</td>\n",
       "      <td>17:20:00</td>\n",
       "      <td>2014-05-03 05:20:00</td>\n",
       "      <td>2014-05-03</td>\n",
       "      <td>02:24:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-09-18 12:00:00</td>\n",
       "      <td>2025-04-08 04:35:01</td>\n",
       "      <td>Young Adult (18-34)</td>\n",
       "      <td>Vehicle</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Night (9pm-5am)</td>\n",
       "      <td>2014</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>150562049</td>\n",
       "      <td>37.777300</td>\n",
       "      <td>-122.419694</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015-06-30</td>\n",
       "      <td>06:00:00</td>\n",
       "      <td>2015-06-30 06:00:00</td>\n",
       "      <td>2015-06-28</td>\n",
       "      <td>03:52:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-09-18 12:00:00</td>\n",
       "      <td>2025-04-08 04:35:01</td>\n",
       "      <td>Adult (35-64)</td>\n",
       "      <td>Motorcycle</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Night (9pm-5am)</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>140104811</td>\n",
       "      <td>37.778251</td>\n",
       "      <td>-122.419883</td>\n",
       "      <td>2014</td>\n",
       "      <td>2014-02-06</td>\n",
       "      <td>10:20:00</td>\n",
       "      <td>2014-02-06 10:20:00</td>\n",
       "      <td>2014-02-05</td>\n",
       "      <td>02:26:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-09-18 12:00:00</td>\n",
       "      <td>2025-04-08 04:35:01</td>\n",
       "      <td>Adult (35-64)</td>\n",
       "      <td>Pedestrian</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Night (9pm-5am)</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id case_id_fkey   latitude   longitude  collision_year  death_date  \\\n",
       "0          1    140236301  37.710409 -122.404226            2014  2014-03-20   \n",
       "1          2    140755533  37.725476 -122.394243            2014  2014-09-08   \n",
       "2          4    140365546  37.748255 -122.413669            2014  2014-05-03   \n",
       "3         16    150562049  37.777300 -122.419694            2015  2015-06-30   \n",
       "4         17    140104811  37.778251 -122.419883            2014  2014-02-06   \n",
       "\n",
       "  death_time       death_datetime collision_date collision_time  ...  \\\n",
       "0   11:21:00  2014-03-20 11:21:00     2014-03-20            NaN  ...   \n",
       "1   16:38:00  2014-09-08 04:38:00     2014-09-08       05:10:00  ...   \n",
       "2   17:20:00  2014-05-03 05:20:00     2014-05-03       02:24:00  ...   \n",
       "3   06:00:00  2015-06-30 06:00:00     2015-06-28       03:52:00  ...   \n",
       "4   10:20:00  2014-02-06 10:20:00     2014-02-05       02:26:00  ...   \n",
       "\n",
       "            data_as_of       data_loaded_at         age_category  \\\n",
       "0  2024-09-18 12:00:00  2025-04-08 04:35:01         Senior (65+)   \n",
       "1  2024-09-18 12:00:00  2025-04-08 04:35:01         Senior (65+)   \n",
       "2  2024-09-18 12:00:00  2025-04-08 04:35:01  Young Adult (18-34)   \n",
       "3  2024-09-18 12:00:00  2025-04-08 04:35:01        Adult (35-64)   \n",
       "4  2024-09-18 12:00:00  2025-04-08 04:35:01        Adult (35-64)   \n",
       "\n",
       "  collision_category collision_hour           time_of_day  \\\n",
       "0         Pedestrian            NaN  Afternoon (12pm-5pm)   \n",
       "1         Pedestrian            5.0    Morning (5am-12pm)   \n",
       "2            Vehicle            2.0       Night (9pm-5am)   \n",
       "3         Motorcycle            3.0       Night (9pm-5am)   \n",
       "4         Pedestrian            2.0       Night (9pm-5am)   \n",
       "\n",
       "  collision_year_clean  collision_month  collision_day  day_of_week  \n",
       "0                 2014                3             20     Thursday  \n",
       "1                 2014                9              8       Monday  \n",
       "2                 2014                5              3     Saturday  \n",
       "3                 2015                6             28       Sunday  \n",
       "4                 2014                2              5    Wednesday  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05933d4f-558e-493f-8bd7-e6d692da1444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unique_id', 'case_id_fkey', 'latitude', 'longitude', 'collision_year',\n",
       "       'death_date', 'death_time', 'death_datetime', 'collision_date',\n",
       "       'collision_time', 'collision_datetime', 'location', 'age', 'sex',\n",
       "       'deceased', 'collision_type', 'street_type', 'on_vz_hin_2017',\n",
       "       'in_coc_2018', 'publish', 'on_vz_hin_2022', 'in_epa_2021', 'point',\n",
       "       'analysis_neighborhood', 'supervisor_district', 'police_district',\n",
       "       'data_as_of', 'data_loaded_at', 'age_category', 'collision_category',\n",
       "       'collision_hour', 'time_of_day', 'collision_year_clean',\n",
       "       'collision_month', 'collision_day', 'day_of_week'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21defb5e-b64d-4f57-97f7-4c7de8643d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deceased\n",
       "Pedestrian                      190\n",
       "Motorcyclist                     45\n",
       "Driver                           41\n",
       "Bicyclist                        24\n",
       "Passenger                        17\n",
       "Standup Powered Device Rider      9\n",
       "Exterior Passenger                3\n",
       "Moped                             2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['deceased'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "084cee02-f805-4292-b287-0c4c8a7d10ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collision_type\n",
       "Pedestrian vs Motor Vehicle               174\n",
       "Motor Vehicle Collision                    54\n",
       "Motorcycle vs Motor Vehicle                26\n",
       "Motorcycle Collision                       17\n",
       "Bicycle vs Motor Vehicle                   16\n",
       "Motor Vehicle Collision (solo)              6\n",
       "Pedestrian vs LRV                           5\n",
       "Bicycle Collision                           5\n",
       "Motor Vehicle & Pedestrian                  4\n",
       "e-Scooter Collision                         3\n",
       "Pedestrian vs Motorcycle                    3\n",
       "e-Scooter vs. Vehicle                       3\n",
       "e-Scooter vs Motor Vehicle                  2\n",
       "Moped vs Motor Vehicle                      2\n",
       "Dirt Bike Collision                         1\n",
       "Pedestrian Collision                        1\n",
       "e-Scooter vs. Pedestrian                    1\n",
       "Pedestrian vs Bus (Paratransit)             1\n",
       "Pedestrian vs. MUNI                         1\n",
       "Pedestrian vs Bicyclist                     1\n",
       "Motorcycle vs Truck                         1\n",
       "LRV Collision                               1\n",
       "Standup Powered Device & Motor Vehicle      1\n",
       "Pedestrian vs Cable Car                     1\n",
       "Bicycle vs MUNI                             1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['collision_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d75e075-2296-44b9-a9dd-812e049df3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collision_category\n",
       "Pedestrian    192\n",
       "Vehicle        73\n",
       "Motorcycle     44\n",
       "Bicycle        22\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['collision_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68281543-9fb9-4721-9f56-e7c7e50fb946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "street_type\n",
       "City Street    331\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['street_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8229cd2e-70a6-47eb-9569-469746f97b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating target variable...\n",
      "Using direct approach to create high_risk target variable\n",
      "\n",
      "Preparing features...\n"
     ]
    }
   ],
   "source": [
    "# Create target variable: high-risk vs low-risk areas\n",
    "print(\"\\nCreating target variable...\")\n",
    "\n",
    "# Define high-risk based on collision category and time of day\n",
    "# This avoids a merge operation that was causing issues\n",
    "print(\"Using direct approach to create high_risk target variable\")\n",
    "# 1 if victim is pedestrian, 0 if not\n",
    "df['pedestrian_deceased'] = [1 if row == 'Pedestrian' else 0 for row in df['deceased']] \n",
    "\n",
    "# Verify the column exists and has appropriate values\n",
    "# print(f\"High risk incidents: {df['pedestrian_deceased'].sum()} out of {len(df)} total records\")\n",
    "# print(f\"Percentage high risk: {df['pedestrian_deceased'].mean() * 100:.2f}%\")\n",
    "\n",
    "# Select features for the model\n",
    "print(\"\\nPreparing features...\")\n",
    "features = [\n",
    "    'collision_hour', 'collision_month', 'analysis_neighborhood',\n",
    "    'day_of_week',\n",
    "]\n",
    "\n",
    "# Check if all features exist\n",
    "missing_features = [f for f in features if f not in df.columns]\n",
    "if missing_features:\n",
    "    print(f\"Warning: Missing features: {missing_features}\")\n",
    "    # Remove missing features from the list\n",
    "    features = [f for f in features if f in df.columns]\n",
    "    print(f\"Proceeding with available features: {features}\")\n",
    "\n",
    "# Handle missing values in features\n",
    "for feature in features:\n",
    "    if df[feature].dtype == 'object':\n",
    "        df[feature] = df[feature].fillna('Unknown')\n",
    "    else:\n",
    "        df[feature] = df[feature].fillna(df[feature].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b1921d5-0e09-4e31-a589-2af835f0228a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution:\n",
      "pedestrian_deceased\n",
      "1    190\n",
      "0    141\n",
      "Name: count, dtype: int64\n",
      "Class balance: 57.40% high risk\n"
     ]
    }
   ],
   "source": [
    "# Prepare X and y\n",
    "X = df[features]\n",
    "y = df['pedestrian_deceased']\n",
    "\n",
    "# Print class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"Class balance: {y.mean() * 100:.2f}% high risk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f64427c-0ba0-49c9-9c52-9acdec027b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting data into train and test sets...\n",
      "Class distribution in training set:\n",
      "pedestrian_deceased\n",
      "1    142\n",
      "0    106\n",
      "Name: count, dtype: int64\n",
      "Training set class balance: 57.26% high risk\n",
      "\n",
      "Building preprocessing pipeline...\n",
      "Numerical features: ['collision_hour', 'collision_month']\n",
      "Categorical features: ['analysis_neighborhood', 'day_of_week']\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "print(\"\\nSplitting data into train and test sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "# Check class distribution in training set\n",
    "print(\"Class distribution in training set:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "print(f\"Training set class balance: {y_train.mean() * 100:.2f}% high risk\")\n",
    "\n",
    "# Define preprocessing for numerical and categorical features\n",
    "print(\"\\nBuilding preprocessing pipeline...\")\n",
    "numerical_features = [f for f in features if df[f].dtype != 'object']\n",
    "categorical_features = [f for f in features if df[f].dtype == 'object']\n",
    "\n",
    "print(f\"Numerical features: {numerical_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "273f9fc8-6089-4554-be52-da13693e3603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the model...\n",
      "Model training completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Create and train the model\n",
    "print(\"\\nTraining the model...\")\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "try:\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Model training completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during model training: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d224932-76a5-4782-89a3-aaef16623054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model performance...\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.29      0.34        35\n",
      "           1       0.58      0.73      0.65        48\n",
      "\n",
      "    accuracy                           0.54        83\n",
      "   macro avg       0.51      0.51      0.50        83\n",
      "weighted avg       0.52      0.54      0.52        83\n",
      "\n",
      "ROC AUC Score: 0.4979\n",
      "\n",
      "Generating SHAP values for feature importance...\n",
      "Error generating SHAP values: Cannot cast ufunc 'isnan' input from dtype('O') to dtype('bool') with casting rule 'same_kind'\n",
      "Continuing without SHAP values\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print(\"\\nEvaluating model performance...\")\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "try:\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not calculate ROC AUC: {e}\")\n",
    "\n",
    "# Generate SHAP values for feature importance\n",
    "print(\"\\nGenerating SHAP values for feature importance...\")\n",
    "try:\n",
    "    # Get the preprocessed test data\n",
    "    X_test_processed = model.named_steps['preprocessor'].transform(X_test)\n",
    "    \n",
    "    # Create a SHAP explainer\n",
    "    explainer = shap.TreeExplainer(model.named_steps['classifier'])\n",
    "    shap_values = explainer.shap_values(X_test_processed)\n",
    "    \n",
    "    # Get feature names after preprocessing\n",
    "    cat_feature_names = []\n",
    "    for i, col in enumerate(categorical_features):\n",
    "        cats = model.named_steps['preprocessor'].transformers_[1][1].categories_[i]\n",
    "        for cat in cats:\n",
    "            cat_feature_names.append(f\"{col}_{cat}\")\n",
    "    \n",
    "    feature_names = numerical_features + cat_feature_names\n",
    "    \n",
    "    print(f\"Generated SHAP values with {len(feature_names)} features\")\n",
    "    \n",
    "    # Save feature names for the Streamlit app\n",
    "    with open('../data/feature_names.pkl', 'wb') as f:\n",
    "        pickle.dump(feature_names, f)\n",
    "    \n",
    "    # Save SHAP values for the Streamlit app\n",
    "    with open('../data/shap_values.pkl', 'wb') as f:\n",
    "        pickle.dump((shap_values, X_test_processed), f)\n",
    "    \n",
    "    print(\"SHAP values saved successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error generating SHAP values: {e}\")\n",
    "    print(\"Continuing without SHAP values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "548bdcdf-6177-443c-acac-e7b6deef13a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25430493, 0.10863604, 0.03630204, 0.00554248, 0.00970569,\n",
       "       0.02961184, 0.00567125, 0.0244189 , 0.00240798, 0.01398206,\n",
       "       0.0131362 , 0.        , 0.        , 0.        , 0.0076557 ,\n",
       "       0.01690189, 0.03356704, 0.02497248, 0.02440979, 0.01966381,\n",
       "       0.00503209, 0.00472191, 0.00181731, 0.01916684, 0.00239362,\n",
       "       0.03559732, 0.01596543, 0.00166534, 0.0241568 , 0.01219856,\n",
       "       0.00264316, 0.00568782, 0.00375164, 0.02840229, 0.00283753,\n",
       "       0.        , 0.01315629, 0.00182082, 0.01354776, 0.00279183,\n",
       "       0.031889  , 0.02882393, 0.02995767, 0.01227627, 0.06880862])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['classifier'].feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f432023-2d6d-43ae-a65e-888539c19456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num__collision_hour 0.25430492930568904\n",
      "num__collision_month 0.10863604360239083\n",
      "cat__analysis_neighborhood_Bayview Hunters Point 0.036302037520566495\n",
      "cat__analysis_neighborhood_Bernal Heights 0.005542484269442871\n",
      "cat__analysis_neighborhood_Castro/Upper Market 0.009705688210794856\n",
      "cat__analysis_neighborhood_Chinatown 0.029611843245568762\n",
      "cat__analysis_neighborhood_Excelsior 0.00567124852589609\n",
      "cat__analysis_neighborhood_Financial District/South Beach 0.024418903249128487\n",
      "cat__analysis_neighborhood_Glen Park 0.0024079790458254773\n",
      "cat__analysis_neighborhood_Golden Gate Park 0.013982063408217765\n",
      "cat__analysis_neighborhood_Haight Ashbury 0.013136203161769758\n",
      "cat__analysis_neighborhood_Hayes Valley 0.0\n",
      "cat__analysis_neighborhood_Inner Richmond 0.0\n",
      "cat__analysis_neighborhood_Inner Sunset 0.0\n",
      "cat__analysis_neighborhood_Japantown 0.007655696829131034\n",
      "cat__analysis_neighborhood_Lakeshore 0.01690189020280588\n",
      "cat__analysis_neighborhood_Lone Mountain/USF 0.03356704002000638\n",
      "cat__analysis_neighborhood_Marina 0.024972480704495502\n",
      "cat__analysis_neighborhood_McLaren Park 0.024409786716258652\n",
      "cat__analysis_neighborhood_Mission 0.01966380922343623\n",
      "cat__analysis_neighborhood_Mission Bay 0.005032090006148757\n",
      "cat__analysis_neighborhood_Nob Hill 0.00472190985450553\n",
      "cat__analysis_neighborhood_Noe Valley 0.0018173073244062796\n",
      "cat__analysis_neighborhood_North Beach 0.019166839424655736\n",
      "cat__analysis_neighborhood_Oceanview/Merced/Ingleside 0.0023936213420331014\n",
      "cat__analysis_neighborhood_Outer Mission 0.035597324199702664\n",
      "cat__analysis_neighborhood_Outer Richmond 0.0159654329756771\n",
      "cat__analysis_neighborhood_Pacific Heights 0.001665342132078065\n",
      "cat__analysis_neighborhood_Portola 0.024156802462027095\n",
      "cat__analysis_neighborhood_Potrero Hill 0.012198563039780444\n",
      "cat__analysis_neighborhood_Russian Hill 0.002643163997199943\n",
      "cat__analysis_neighborhood_South of Market 0.0056878159655198044\n",
      "cat__analysis_neighborhood_Sunset/Parkside 0.0037516369702725928\n",
      "cat__analysis_neighborhood_Tenderloin 0.02840229150441555\n",
      "cat__analysis_neighborhood_Treasure Island 0.0028375316405669774\n",
      "cat__analysis_neighborhood_Visitacion Valley 0.0\n",
      "cat__analysis_neighborhood_West of Twin Peaks 0.013156288705486364\n",
      "cat__analysis_neighborhood_Western Addition 0.001820823889487324\n",
      "cat__day_of_week_Friday 0.013547763386227112\n",
      "cat__day_of_week_Monday 0.0027918268843759417\n",
      "cat__day_of_week_Saturday 0.031889001909331756\n",
      "cat__day_of_week_Sunday 0.028823930972611447\n",
      "cat__day_of_week_Thursday 0.02995767148986058\n",
      "cat__day_of_week_Tuesday 0.012276271397982454\n",
      "cat__day_of_week_Wednesday 0.06880862128422326\n"
     ]
    }
   ],
   "source": [
    "model[:-1].get_feature_names_out()\n",
    "for name, importance in zip(model[:-1].get_feature_names_out(), model['classifier'].feature_importances_):\n",
    "    print(name, importance)\n",
    "# put this into a DataFrame so that you won't have to use SHAP\n",
    "\n",
    "\n",
    "            \n",
    "# Create DataFrame from the tuples\n",
    "df1 = pd.DataFrame(feature_importances, columns = ['feature', 'importance'])\n",
    "\n",
    "# Save to CSV\n",
    "df1.to_csv('../data/feature_importances.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d83225c-b9ad-4b18-8c92-acfb292c5a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "print(\"\\nSaving model and data for Streamlit app...\")\n",
    "try:\n",
    "    with open('../data/fatality_pedestrian_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(\"Model saved successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Save a sample of the data for the Streamlit app\n",
    "try:\n",
    "    df_sample = df.sample(frac=1.0, random_state=42)\n",
    "    df_sample.to_csv('../data/fatality_data_processed.csv', index=False)\n",
    "    print(\"Processed data saved successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving processed data: {e}\")\n",
    "\n",
    "print(\"\\nModel training complete. Files saved for Streamlit app.\")\n",
    "print(\"You can now run the Streamlit app with: streamlit run app.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47dd38f-a231-457d-99d9-116ca0c7673b",
   "metadata": {},
   "source": [
    "---\n",
    "# Next: [Findings and Technical Report](/04-findings-tech-report.ipynb)\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
