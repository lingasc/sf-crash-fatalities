{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "081925ab-eda3-4bc0-a8ef-14e7fff17bc3",
   "metadata": {},
   "source": [
    "# 03: Model Training, Fitting, and Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e619b096-0997-47a7-8354-0dee156b10b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import shap     # a game theoretical approach to explaining model output https://shap.readthedocs.io/en/latest/index.html\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbd58aef-a89f-422e-aa72-2a42546242ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training process...\n",
      "Error loading data: [Errno 2] No such file or directory: './data/cleaned_fatalities.csv'\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting model training process...\")\n",
    "\n",
    "# Load the cleaned data\n",
    "try:\n",
    "    df = pd.read_csv('./data/cleaned_fatalities.csv')\n",
    "    print(f\"Successfully loaded data with {len(df)} rows and {len(df.columns)} columns\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdb21b79-0d27-4b8e-bc16-c7773af714d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Validate required columns\u001b[39;00m\n\u001b[1;32m      2\u001b[0m required_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcollision_category\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_of_day\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m missing_columns \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m required_columns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_columns:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Missing required columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_columns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Validate required columns\n",
    "required_columns = ['latitude', 'longitude', 'collision_category', 'time_of_day']\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    print(f\"Error: Missing required columns: {missing_columns}\")\n",
    "    print(\"Available columns:\", df.columns.tolist())\n",
    "    exit(1)\n",
    "\n",
    "print(\"Data validation passed. Beginning feature engineering...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f901d9c2-a8ca-43ef-bb7f-62b3664d6160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# Add day of week with robust error handling\n",
    "print(\"\\nCreating day_of_week column...\")\n",
    "\n",
    "# First try using collision_date\n",
    "if 'collision_date' in df.columns:\n",
    "    try:\n",
    "        df['collision_date'] = pd.to_datetime(df['collision_date'], errors='coerce')\n",
    "        if df['collision_date'].notna().sum() > len(df) * 0.5:  # If more than 50% valid dates\n",
    "            df['day_of_week'] = df['collision_date'].dt.day_name()\n",
    "            print(\"Successfully created day_of_week from collision_date\")\n",
    "        else:\n",
    "            raise ValueError(\"Too many NaN values in collision_date\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error using collision_date: {e}\")\n",
    "        # Try alternative method\n",
    "        if all(col in df.columns for col in ['collision_year_clean', 'collision_month', 'collision_day']):\n",
    "            try:\n",
    "                df['synthetic_date'] = pd.to_datetime(\n",
    "                    df['collision_year_clean'].astype(str) + '-' + \n",
    "                    df['collision_month'].astype(str) + '-' + \n",
    "                    df['collision_day'].astype(str),\n",
    "                    errors='coerce'\n",
    "                )\n",
    "                df['day_of_week'] = df['synthetic_date'].dt.day_name()\n",
    "                print(\"Successfully created day_of_week from synthetic date\")\n",
    "            except Exception as e2:\n",
    "                print(f\"Error creating synthetic date: {e2}\")\n",
    "                # Fall back to default\n",
    "                days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "                df['day_of_week'] = [days[i % 7] for i in range(len(df))]\n",
    "                print(\"Created default day_of_week\")\n",
    "        else:\n",
    "            # Fall back to default\n",
    "            days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "            df['day_of_week'] = [days[i % 7] for i in range(len(df))]\n",
    "            print(\"Created default day_of_week\")\n",
    "else:\n",
    "    # If collision_date doesn't exist, create a default day_of_week\n",
    "    print(\"Warning: collision_date column not found\")\n",
    "    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    df['day_of_week'] = [days[i % 7] for i in range(len(df))]\n",
    "    print(\"Created default day_of_week\")# Verify day_of_week column\n",
    "print(f\"day_of_week column exists: {'day_of_week' in df.columns}\")\n",
    "if 'day_of_week' in df.columns:\n",
    "    print(f\"Unique values in day_of_week: {df['day_of_week'].unique()}\")\n",
    "    print(f\"NaN values in day_of_week: {df['day_of_week'].isna().sum()}\")\n",
    "    # Fill any NaN values\n",
    "    df['day_of_week'] = df['day_of_week'].fillna('Monday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8229cd2e-70a6-47eb-9569-469746f97b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target variable: high-risk vs low-risk areas\n",
    "print(\"\\nCreating target variable...\")\n",
    "\n",
    "# SIMPLIFIED APPROACH: Define high-risk based on collision category and time of day\n",
    "# This avoids the merge operation that was causing issues\n",
    "print(\"Using direct approach to create high_risk target variable\")\n",
    "df['high_risk'] = ((df['collision_category'] == 'Pedestrian') | \n",
    "                  (df['time_of_day'] == 'Night (9pm-5am)')).astype(int)\n",
    "\n",
    "# Verify the column exists and has appropriate values\n",
    "print(f\"High risk incidents: {df['high_risk'].sum()} out of {len(df)} total records\")\n",
    "print(f\"Percentage high risk: {df['high_risk'].mean() * 100:.2f}%\")\n",
    "\n",
    "# Select features for the model\n",
    "print(\"\\nPreparing features...\")\n",
    "features = [\n",
    "    'latitude', 'longitude', 'collision_hour', 'collision_month',\n",
    "    'time_of_day', 'day_of_week', 'collision_category'\n",
    "]\n",
    "\n",
    "# Check if all features exist\n",
    "missing_features = [f for f in features if f not in df.columns]\n",
    "if missing_features:\n",
    "    print(f\"Warning: Missing features: {missing_features}\")\n",
    "    # Remove missing features from the list\n",
    "    features = [f for f in features if f in df.columns]\n",
    "    print(f\"Proceeding with available features: {features}\")\n",
    "\n",
    "# Handle missing values in features\n",
    "for feature in features:\n",
    "    if df[feature].dtype == 'object':\n",
    "        df[feature] = df[feature].fillna('Unknown')\n",
    "    else:\n",
    "        df[feature] = df[feature].fillna(df[feature].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1921d5-0e09-4e31-a589-2af835f0228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "X = df[features]\n",
    "y = df['high_risk']\n",
    "\n",
    "# Print class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"Class balance: {y.mean() * 100:.2f}% high risk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f64427c-0ba0-49c9-9c52-9acdec027b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "print(\"\\nSplitting data into train and test sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "# Check class distribution in training set\n",
    "print(\"Class distribution in training set:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "print(f\"Training set class balance: {y_train.mean() * 100:.2f}% high risk\")\n",
    "\n",
    "# Define preprocessing for numerical and categorical features\n",
    "print(\"\\nBuilding preprocessing pipeline...\")\n",
    "numerical_features = [f for f in features if df[f].dtype != 'object']\n",
    "categorical_features = [f for f in features if df[f].dtype == 'object']\n",
    "\n",
    "print(f\"Numerical features: {numerical_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273f9fc8-6089-4554-be52-da13693e3603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "print(\"\\nTraining the model...\")\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "try:\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Model training completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during model training: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d224932-76a5-4782-89a3-aaef16623054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"\\nEvaluating model performance...\")\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "try:\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not calculate ROC AUC: {e}\")\n",
    "\n",
    "# Generate SHAP values for feature importance\n",
    "print(\"\\nGenerating SHAP values for feature importance...\")\n",
    "try:\n",
    "    # Get the preprocessed test data\n",
    "    X_test_processed = model.named_steps['preprocessor'].transform(X_test)\n",
    "    \n",
    "    # Create a SHAP explainer\n",
    "    explainer = shap.TreeExplainer(model.named_steps['classifier'])\n",
    "    shap_values = explainer.shap_values(X_test_processed)\n",
    "    \n",
    "    # Get feature names after preprocessing\n",
    "    cat_feature_names = []\n",
    "    for i, col in enumerate(categorical_features):\n",
    "        cats = model.named_steps['preprocessor'].transformers_[1][1].categories_[i]\n",
    "        for cat in cats:\n",
    "            cat_feature_names.append(f\"{col}_{cat}\")\n",
    "    \n",
    "    feature_names = numerical_features + cat_feature_names\n",
    "    \n",
    "    print(f\"Generated SHAP values with {len(feature_names)} features\")\n",
    "    \n",
    "    # Save feature names for the Streamlit app\n",
    "    with open('./data/feature_names.pkl', 'wb') as f:\n",
    "        pickle.dump(feature_names, f)\n",
    "    \n",
    "    # Save SHAP values for the Streamlit app\n",
    "    with open('./data/shap_values.pkl', 'wb') as f:\n",
    "        pickle.dump((shap_values, X_test_processed), f)\n",
    "    \n",
    "    print(\"SHAP values saved successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error generating SHAP values: {e}\")\n",
    "    print(\"Continuing without SHAP values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d83225c-b9ad-4b18-8c92-acfb292c5a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "print(\"\\nSaving model and data for Streamlit app...\")\n",
    "try:\n",
    "    with open('./data/fatality_risk_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(\"Model saved successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Save a sample of the data for the Streamlit app\n",
    "try:\n",
    "    df_sample = df.sample(frac=1.0, random_state=42)\n",
    "    df_sample.to_csv('./data/fatality_data_processed.csv', index=False)\n",
    "    print(\"Processed data saved successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving processed data: {e}\")\n",
    "\n",
    "print(\"\\nModel training complete. Files saved for Streamlit app.\")\n",
    "print(\"You can now run the Streamlit app with: streamlit run app.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
